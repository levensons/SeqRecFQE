{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.rl.cql_dqn import *\n",
    "from src.rl.rec_replay_buffer import RecReplayBuffer\n",
    "from RECE.data import get_dataset, data_to_sequences, SequentialDataset\n",
    "from RECE.train import prepare_sasrec_model, train_sasrec_epoch, downvote_seen_items, sasrec_model_scoring, topn_recommendations, model_evaluate\n",
    "from cql_utils import prepare_cql_model\n",
    "from RECE.rl_ope.utils import prepare_svd\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from clearml import Task, Logger\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec_config = dict(\n",
    "    manual_seed = 123,\n",
    "    sampler_seed = 123,\n",
    "    init_emb_svd = None,\n",
    "    lin_layer_dim = -1,\n",
    "    num_epochs = 5, #3 10 22 100&dropout0.9&hd32&bs1000\n",
    "    maxlen = 100,\n",
    "    hidden_units = 128,\n",
    "    dropout_rate = 0.3,\n",
    "    num_blocks = 2,\n",
    "    num_heads = 2,\n",
    "    batch_size = 128,\n",
    "    learning_rate = 1e-3,\n",
    "    fwd_type = 'bce',\n",
    "    l2_emb = 0,\n",
    "    patience = 10,\n",
    "    skip_epochs = 1,\n",
    "    n_neg_samples=600,\n",
    "    sampling='without_rep'\n",
    ")\n",
    "\n",
    "config = TrainConfig(\n",
    "    orthogonal_init = True,\n",
    "    q_n_hidden_layers = 1,\n",
    "    qf_lr = 3e-4,\n",
    "    batch_size=128,\n",
    "    device=\"cuda:0\",\n",
    "    bc_steps=100000,\n",
    "    cql_alpha=100.0,\n",
    "\n",
    "    env=\"MovieLens\",\n",
    "    project= \"CQL-SASREC\",\n",
    "    group= \"CQL-SASREC\",\n",
    "    name= \"CQL\",\n",
    "    #cql_negative_samples = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6a5c036f-c8ac-4e07-abb6-02079fa2c333) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CQL-MovieLens-f8b93dc3</strong> at: <a href='https://wandb.ai/levenson/CQL-SASREC/runs/6a5c036f-c8ac-4e07-abb6-02079fa2c333' target=\"_blank\">https://wandb.ai/levenson/CQL-SASREC/runs/6a5c036f-c8ac-4e07-abb6-02079fa2c333</a><br/> View project at: <a href='https://wandb.ai/levenson/CQL-SASREC' target=\"_blank\">https://wandb.ai/levenson/CQL-SASREC</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241129_113745-6a5c036f-c8ac-4e07-abb6-02079fa2c333/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6a5c036f-c8ac-4e07-abb6-02079fa2c333). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/sasrec_cql/wandb/run-20241129_162008-06a585db-c695-48d4-bf8c-bf50001cfcc1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/levenson/CQL-SASREC/runs/06a585db-c695-48d4-bf8c-bf50001cfcc1' target=\"_blank\">CQL-MovieLens-d8bd6124</a></strong> to <a href='https://wandb.ai/levenson/CQL-SASREC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/levenson/CQL-SASREC' target=\"_blank\">https://wandb.ai/levenson/CQL-SASREC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/levenson/CQL-SASREC/runs/06a585db-c695-48d4-bf8c-bf50001cfcc1' target=\"_blank\">https://wandb.ai/levenson/CQL-SASREC/runs/06a585db-c695-48d4-bf8c-bf50001cfcc1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"API KEY\" # Change to your W&B profile if you need it\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "\n",
    "seed = config.seed\n",
    "set_seed(seed)\n",
    "wandb_init(asdict(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_temp = pd.read_csv('./RECE/training_temp.csv')\n",
    "testset_valid_temp_cut = pd.read_csv('./RECE/testset_valid_temp_cut.csv')\n",
    "holdout_valid_temp_cut = pd.read_csv('./RECE/holdout_valid_temp_cut.csv')\n",
    "data_description_temp = {'users': 'userid',\n",
    " 'items': 'itemid',\n",
    " 'order': 'timestamp',\n",
    " 'n_users': training_temp.userid.nunique(),\n",
    " 'n_items': training_temp.itemid.max()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_temp['rating'] = np.ones(training_temp.shape[0])\n",
    "\n",
    "# item_embs = prepare_svd(training_temp, data_description_temp, rank=128, device=device)\n",
    "# item_embs = torch.load(\"./RECE/saved_models/item_embs.pt\", map_location=torch.device(device))\n",
    "item_embs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sasrec_model, sampler, n_batches, optimizers = prepare_sasrec_model(sasrec_config, training_temp, data_description_temp, device, item_embs)\n",
    "sasrec_model.load_state_dict(torch.load(\"./RECE/saved_models/model_e0_nonsvd.pt\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = log = None\n",
    "\n",
    "def pretrain(model, config, data_description, testset_valid, holdout_valid):   \n",
    "    losses = {}\n",
    "    metrics = {}\n",
    "    ndcg = {}\n",
    "    best_ndcg = 0\n",
    "    wait = 0\n",
    "\n",
    "    start_time = time()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "    checkpt_name = uuid.uuid4().hex\n",
    "    if not os.path.exists('./checkpt'):\n",
    "        os.mkdir('./checkpt')\n",
    "    \n",
    "    checkpt_path = os.path.join('./checkpt', f'{checkpt_name}.chkpt')\n",
    "\n",
    "    for epoch in (range(config['num_epochs'])):\n",
    "        losses[epoch] = train_sasrec_epoch(\n",
    "            model, n_batches, config['l2_emb'], sampler, optimizers, device\n",
    "        )\n",
    "        if epoch % config['skip_epochs'] == 0:\n",
    "            val_scores = sasrec_model_scoring(model, testset_valid, data_description, device)\n",
    "            downvote_seen_items(val_scores, testset_valid, data_description)\n",
    "            val_recs = topn_recommendations(val_scores, topn=10)\n",
    "            val_metrics = model_evaluate(val_recs, holdout_valid, data_description)\n",
    "            metrics[epoch] = val_metrics\n",
    "            ndcg_ = val_metrics['ndcg@10']\n",
    "            ndcg[epoch] = ndcg_\n",
    "\n",
    "            print(f'Epoch {epoch}, NDCG@10: {ndcg_}')\n",
    "            \n",
    "            if task and (epoch % 5 == 0):\n",
    "                log.report_scalar(\"Loss\", series='Val', iteration=epoch, value=np.mean(losses[epoch]))\n",
    "                log.report_scalar(\"NDCG\", series='Val', iteration=epoch, value=ndcg_)\n",
    "\n",
    "            if ndcg_ > best_ndcg:\n",
    "                best_ndcg = ndcg_\n",
    "                torch.save(model.state_dict(), checkpt_path)\n",
    "                wait = 0\n",
    "            elif wait < config['patience'] // config['skip_epochs'] + 1:\n",
    "                wait += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    training_time_sec = time() - start_time\n",
    "    full_peak_training_memory_bytes = torch.cuda.max_memory_allocated()\n",
    "    peak_training_memory_bytes = torch.cuda.max_memory_allocated() - start_memory\n",
    "    training_epoches = len(losses)\n",
    "    \n",
    "    model.load_state_dict(torch.load(checkpt_path))\n",
    "    os.remove(checkpt_path)\n",
    "\n",
    "    print()\n",
    "    print('Peak training memory, mb:', round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "    print('Training epoches:', training_epoches)\n",
    "    print('Training time, m:', round(training_time_sec/ 60., 2))\n",
    "    \n",
    "    if task:\n",
    "        ind_max = np.argmax(list(ndcg.values())) * config['skip_epochs']\n",
    "        for metric_name, metric_value in metrics[ind_max].items():\n",
    "            log.report_single_value(name=f'val_{metric_name}', value=round(metric_value, 4))\n",
    "        log.report_single_value(name='train_peak_mem_mb', value=round(peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='full_train_peak_mem_mb', value=round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='train_epoches', value=training_epoches)\n",
    "        log.report_single_value(name='train_time_m', value=round(training_time_sec/ 60., 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, NDCG@10: 0.03848937844673427\n",
      "Epoch 1, NDCG@10: 0.05149334477378582\n",
      "Epoch 2, NDCG@10: 0.05857522232397836\n",
      "Epoch 3, NDCG@10: 0.06452599268944262\n",
      "Epoch 4, NDCG@10: 0.06672220069277887\n",
      "Epoch 5, NDCG@10: 0.07118678613351874\n",
      "Epoch 6, NDCG@10: 0.0721333077903089\n",
      "Epoch 7, NDCG@10: 0.07349210004155463\n",
      "Epoch 8, NDCG@10: 0.07460195396266388\n",
      "Epoch 9, NDCG@10: 0.07631886320983909\n",
      "Epoch 10, NDCG@10: 0.07870041077568492\n",
      "Epoch 11, NDCG@10: 0.07840747655850919\n",
      "Epoch 12, NDCG@10: 0.08176249038116373\n",
      "Epoch 13, NDCG@10: 0.08351351643305181\n",
      "Epoch 14, NDCG@10: 0.08130206912864789\n",
      "Epoch 15, NDCG@10: 0.08617508134806617\n",
      "Epoch 16, NDCG@10: 0.08613999445791265\n",
      "Epoch 17, NDCG@10: 0.08662879402498772\n",
      "Epoch 18, NDCG@10: 0.08900912471124398\n",
      "Epoch 19, NDCG@10: 0.08556771803440227\n",
      "Epoch 20, NDCG@10: 0.08789222244846515\n",
      "Epoch 21, NDCG@10: 0.08803378170503412\n",
      "Epoch 22, NDCG@10: 0.08767894004377316\n",
      "Epoch 23, NDCG@10: 0.08954940368583189\n",
      "Epoch 24, NDCG@10: 0.08901741459733727\n",
      "Epoch 25, NDCG@10: 0.08842802463238823\n",
      "Epoch 26, NDCG@10: 0.08809856784458449\n",
      "Epoch 27, NDCG@10: 0.08983486949622847\n",
      "Epoch 28, NDCG@10: 0.09017375904672739\n",
      "Epoch 29, NDCG@10: 0.09172885226186127\n",
      "Epoch 30, NDCG@10: 0.09062277834824606\n",
      "Epoch 31, NDCG@10: 0.09304004774739194\n",
      "Epoch 32, NDCG@10: 0.09142722128490138\n",
      "Epoch 33, NDCG@10: 0.09126689705034531\n",
      "Epoch 34, NDCG@10: 0.09187970328187275\n",
      "Epoch 35, NDCG@10: 0.09357510630331022\n",
      "Epoch 36, NDCG@10: 0.09414334616589026\n",
      "Epoch 37, NDCG@10: 0.09420633381168458\n",
      "Epoch 38, NDCG@10: 0.09326165325419478\n",
      "Epoch 39, NDCG@10: 0.09283572321722353\n",
      "Epoch 40, NDCG@10: 0.0912894608631698\n",
      "Epoch 41, NDCG@10: 0.09618407362343702\n",
      "Epoch 42, NDCG@10: 0.09288364563791375\n",
      "Epoch 43, NDCG@10: 0.0940446760336273\n",
      "Epoch 44, NDCG@10: 0.09707914385714955\n",
      "Epoch 45, NDCG@10: 0.09586974684029147\n",
      "Epoch 46, NDCG@10: 0.09619964765190063\n",
      "Epoch 47, NDCG@10: 0.09547201709654071\n",
      "Epoch 48, NDCG@10: 0.09560537754633737\n",
      "Epoch 49, NDCG@10: 0.09720660578098901\n",
      "Epoch 50, NDCG@10: 0.09168402514027281\n",
      "Epoch 51, NDCG@10: 0.0970344291724318\n",
      "Epoch 52, NDCG@10: 0.09618339831353225\n",
      "Epoch 53, NDCG@10: 0.09845163882045367\n",
      "Epoch 54, NDCG@10: 0.09695926533975971\n",
      "Epoch 55, NDCG@10: 0.0971706639817814\n",
      "Epoch 56, NDCG@10: 0.09424405468690497\n",
      "Epoch 57, NDCG@10: 0.0954235936060042\n",
      "Epoch 58, NDCG@10: 0.09819596239497497\n",
      "Epoch 59, NDCG@10: 0.09839142432902005\n",
      "Epoch 60, NDCG@10: 0.09590733373290619\n",
      "Epoch 61, NDCG@10: 0.09674418632298575\n",
      "Epoch 62, NDCG@10: 0.09755815346127145\n",
      "Epoch 63, NDCG@10: 0.09664979974425225\n",
      "Epoch 64, NDCG@10: 0.09613544613798063\n",
      "Epoch 65, NDCG@10: 0.09642534551672562\n",
      "\n",
      "Peak training memory, mb: 689.1\n",
      "Training epoches: 66\n",
      "Training time, m: 12.31\n"
     ]
    }
   ],
   "source": [
    "pretrain(sasrec_model,\n",
    "         sasrec_config,\n",
    "         data_description_temp,\n",
    "         testset_valid_temp,\n",
    "         holdout_valid_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec_model.fwd_type = 'embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sasrec_model.state_dict(), \"./saved_models/sasrec_svd.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_dim = data_description_temp['n_items']+2\n",
    "action_dim = data_description_temp['n_items']+2\n",
    "\n",
    "replay_buffer = RecReplayBuffer(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    config.buffer_size,\n",
    "    config.device,\n",
    "    sampler\n",
    ")\n",
    "\n",
    "max_action = float(1)\n",
    "\n",
    "if config.checkpoints_path is not None:\n",
    "    print(f\"Checkpoints path: {config.checkpoints_path}\")\n",
    "    os.makedirs(config.checkpoints_path, exist_ok=True)\n",
    "    with open(os.path.join(config.checkpoints_path, \"config.yaml\"), \"w\") as f:\n",
    "        pyrallis.dump(config, f)\n",
    "\n",
    "# Set seeds\n",
    "seed = config.seed\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "q_1 = FullyConnectedQFunction(\n",
    "    128,\n",
    "    action_dim,\n",
    "    config.orthogonal_init,\n",
    "    config.q_n_hidden_layers\n",
    ").to(config.device)\n",
    "\n",
    "q_2 = FullyConnectedQFunction(128, action_dim, config.orthogonal_init, config.q_n_hidden_layers).to(\n",
    "    config.device\n",
    ")\n",
    "q_1_optimizer = torch.optim.Adam(list(q_1.parameters()), config.qf_lr)\n",
    "q_2_optimizer = torch.optim.Adam(list(q_2.parameters()), config.qf_lr)\n",
    "\n",
    "kwargs = {\n",
    "    \"body\": sasrec_model,\n",
    "    \"body_optimizer\": optimizers,\n",
    "    \"q_1\": q_1,\n",
    "    \"q_2\": q_2,\n",
    "    \"q_1_optimizer\": q_1_optimizer,\n",
    "    \"q_2_optimizer\": q_2_optimizer,\n",
    "    \"discount\": config.discount,\n",
    "    \"soft_target_update_rate\": config.soft_target_update_rate,\n",
    "    \"device\": config.device,\n",
    "    # CQL\n",
    "    \"target_entropy\": 1,\n",
    "    \"alpha_multiplier\": config.alpha_multiplier,\n",
    "    \"use_automatic_entropy_tuning\": config.use_automatic_entropy_tuning,\n",
    "    \"backup_entropy\": config.backup_entropy,\n",
    "    \"policy_lr\": config.policy_lr,\n",
    "    \"qf_lr\": config.qf_lr,\n",
    "    \"bc_steps\": config.bc_steps,\n",
    "    \"target_update_period\": config.target_update_period,\n",
    "    \"cql_n_actions\": config.cql_n_actions,\n",
    "    \"cql_importance_sample\": config.cql_importance_sample,\n",
    "    \"cql_lagrange\": config.cql_lagrange,\n",
    "    \"cql_target_action_gap\": config.cql_target_action_gap,\n",
    "    \"cql_temp\": config.cql_temp,\n",
    "    \"cql_alpha\": config.cql_alpha,\n",
    "    \"cql_max_target_backup\": config.cql_max_target_backup,\n",
    "    \"cql_clip_diff_min\": config.cql_clip_diff_min,\n",
    "    \"cql_clip_diff_max\": config.cql_clip_diff_max,\n",
    "    \"cql_negative_samples\": 10\n",
    "}\n",
    "\n",
    "trainer = DQNCQL(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = log = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_agent_epoch():\n",
    "    trainer.q_1.train()\n",
    "    trainer.q_2.train()\n",
    "    trainer.body.train()\n",
    "    losses = []\n",
    "    N = len(sampler)\n",
    "    for t in tqdm(range(N), total=N):\n",
    "        batch = replay_buffer.sample(config.batch_size)\n",
    "        batch = [b.to(config.device) for b in batch]\n",
    "        log_dict = trainer.train(batch)\n",
    "        losses.append(log_dict['loss'])\n",
    "        if t % 100 == 1:\n",
    "            print(f\"Iter {t} of {N}. Train loss: \", np.mean(losses[-100:]))\n",
    "    return np.mean(losses)\n",
    "\n",
    "def agent_model_scoring(data, data_description, device):\n",
    "    trainer.q_1.eval()\n",
    "    trainer.q_2.eval()\n",
    "    trainer.body.eval()\n",
    "    test_sequences = data_to_sequences(data, data_description)\n",
    "    # perform scoring on a user-batch level\n",
    "    scores = []\n",
    "    for _, seq in test_sequences.items():\n",
    "        with torch.no_grad():\n",
    "            body_out = trainer.body.score_with_state(torch.tensor(seq, device=device, dtype=torch.long))[-1]\n",
    "            body_out = body_out.reshape(-1, body_out.shape[-1])\n",
    "            predictions = (q_1(body_out) + q_2(body_out)) / 2.0\n",
    "        scores.append(predictions.detach().cpu().numpy())\n",
    "    return np.concatenate(scores, axis=0)\n",
    "\n",
    "def train_agent(config, data_description, testset_valid, holdout_valid):   \n",
    "    losses = {}\n",
    "    metrics = {}\n",
    "    ndcg = {}\n",
    "    best_ndcg = 0\n",
    "    wait = 0\n",
    "\n",
    "    start_time = time()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "    checkpt_name = uuid.uuid4().hex\n",
    "    if not os.path.exists('./checkpt'):\n",
    "        os.mkdir('./checkpt')\n",
    "    \n",
    "    checkpt_path = os.path.join('./checkpt', f'{checkpt_name}.chkpt')\n",
    "\n",
    "    for epoch in (range(config['num_epochs'])):\n",
    "        losses[epoch] = train_agent_epoch()\n",
    "        wandb.log({\n",
    "            \"train_loss\": losses[epoch]\n",
    "        }, step=trainer.total_it)\n",
    "        if epoch % config['skip_epochs'] == 0:\n",
    "            val_scores = agent_model_scoring(testset_valid, data_description, device)\n",
    "            # downvote_seen_items(val_scores, testset_valid, data_description)\n",
    "            val_recs = topn_recommendations(val_scores, topn=10)\n",
    "            val_metrics = model_evaluate(val_recs, holdout_valid, data_description)\n",
    "            metrics[epoch] = val_metrics\n",
    "            ndcg_ = val_metrics['ndcg@10']\n",
    "            ndcg[epoch] = ndcg_\n",
    "\n",
    "            print(f'Epoch {epoch}, NDCG@10: {ndcg_}')\n",
    "            wandb.log({\n",
    "                \"valid NDCG@10\": ndcg_\n",
    "            }, step=trainer.total_it)\n",
    "            \n",
    "            if task and (epoch % 5 == 0):\n",
    "                log.report_scalar(\"Loss\", series='Val', iteration=epoch, value=np.mean(losses[epoch]))\n",
    "                log.report_scalar(\"NDCG\", series='Val', iteration=epoch, value=ndcg_)\n",
    "\n",
    "            if ndcg_ > best_ndcg:\n",
    "                best_ndcg = ndcg_\n",
    "                #torch.save(model.state_dict(), checkpt_path)\n",
    "                wait = 0\n",
    "            elif wait < config['patience'] // config['skip_epochs'] + 1:\n",
    "                wait += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    training_time_sec = time() - start_time\n",
    "    full_peak_training_memory_bytes = torch.cuda.max_memory_allocated()\n",
    "    peak_training_memory_bytes = torch.cuda.max_memory_allocated() - start_memory\n",
    "    training_epoches = len(losses)\n",
    "    \n",
    "    #model.load_state_dict(torch.load(checkpt_path))\n",
    "    #trainer.load_state_dict()\n",
    "    #os.remove(checkpt_path)\n",
    "\n",
    "    print()\n",
    "    print('Peak training memory, mb:', round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "    print('Training epoches:', training_epoches)\n",
    "    print('Training time, m:', round(training_time_sec/ 60., 2))\n",
    "    \n",
    "    if task:\n",
    "        ind_max = np.argmax(list(ndcg.values())) * config['skip_epochs']\n",
    "        for metric_name, metric_value in metrics[ind_max].items():\n",
    "            log.report_single_value(name=f'val_{metric_name}', value=round(metric_value, 4))\n",
    "        log.report_single_value(name='train_peak_mem_mb', value=round(peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='full_train_peak_mem_mb', value=round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='train_epoches', value=training_epoches)\n",
    "        log.report_single_value(name='train_time_m', value=round(training_time_sec/ 60., 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8385 [00:17<20:13:14,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 of 8385. Train loss:  2472.2169189453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 102/8385 [15:27<21:37:07,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 101 of 8385. Train loss:  2292.882099609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 112/8385 [16:58<20:53:53,  9.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_244428/996419788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msasrec_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_description_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset_valid_temp_cut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout_valid_temp_cut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_244428/3318740844.py\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(config, data_description, testset_valid, holdout_valid)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         wandb.log({\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_244428/3318740844.py\u001b[0m in \u001b[0;36mtrain_agent_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlog_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sasrec_cql/src/rl/rec_replay_buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensorBatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mseq_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mresume_iteration_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mresume_iteration_cnt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 \u001b[0mreturn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ResumeIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mreturn_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(sasrec_config, data_description_temp, testset_valid_temp_cut, holdout_valid_temp_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04584767205968748\n"
     ]
    }
   ],
   "source": [
    "val_scores = agent_model_scoring(testset_valid_temp_cut, data_description_temp, device)\n",
    "# downvote_seen_items(val_scores, testset_valid_temp_cut, data_description_temp)\n",
    "val_recs = topn_recommendations(val_scores, topn=10)\n",
    "val_metrics = model_evaluate(val_recs, holdout_valid_temp_cut, data_description_temp)\n",
    "ndcg_ = val_metrics['ndcg@10']\n",
    "print(ndcg_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3079e-02,  2.0348e-02,  3.8471e-02,  ...,  4.8996e-02,\n",
       "         -5.2935e-02,  1.6819e-02],\n",
       "        [-2.3606e-02,  2.9397e-02, -1.1186e-02,  ...,  1.8655e-02,\n",
       "          7.7186e-03, -1.1460e-02],\n",
       "        [-1.3016e-02,  1.6516e-02,  1.2311e-02,  ...,  1.8670e-02,\n",
       "         -2.1991e-03, -6.5829e-03],\n",
       "        ...,\n",
       "        [-5.3611e-03, -2.5121e-03,  8.6240e-03,  ..., -7.2626e-03,\n",
       "          3.2784e-04,  2.3791e-02],\n",
       "        [-3.2585e-19, -1.4039e-18, -2.0581e-19,  ..., -1.6371e-18,\n",
       "         -7.8599e-19,  1.0088e-17],\n",
       "        [ 4.9344e-19,  1.3128e-18,  7.4479e-19,  ...,  3.7011e-18,\n",
       "          4.4683e-19, -1.8019e-17]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.state_dict(), \"./saved_models/sasrec_cql.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 1.0, lr = 3-e4, pretrained \n",
    "Epoch 3, NDCG@10: 0.09942672790465602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 100.0, lr = 3-e4, pretrained \n",
    "Epoch 23, NDCG@10: 0.1265350095744121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hr@1': 0.0406,\n",
       " 'mrr@1': 0.0406,\n",
       " 'ndcg@1': 0.0406,\n",
       " 'cov@1': 0.0007146762645679868,\n",
       " 'hr@5': 0.066,\n",
       " 'mrr@5': 0.04819333333333333,\n",
       " 'ndcg@5': 0.05254203703897219,\n",
       " 'cov@5': 0.0038273927662707246,\n",
       " 'hr@10': 0.0876,\n",
       " 'mrr@10': 0.05100626984126984,\n",
       " 'ndcg@10': 0.059457791932285634,\n",
       " 'cov@10': 0.006212517167298825,\n",
       " 'hr@20': 0.115,\n",
       " 'mrr@20': 0.0528829252584361,\n",
       " 'ndcg@20': 0.0663541769020237,\n",
       " 'cov@20': 0.0094371709152592,\n",
       " 'hr@50': 0.1542,\n",
       " 'mrr@50': 0.05413735211098316,\n",
       " 'ndcg@50': 0.07414550340193599,\n",
       " 'cov@50': 0.015270823556762948}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RECE.eval_utils import get_test_scores\n",
    "from RECE.train import prepare_sasrec_model\n",
    "\n",
    "from polara import get_movielens_data \n",
    "from polara.preprocessing.dataframes import reindex, leave_one_out\n",
    "\n",
    "testset = pd.read_csv('RECE/testset.csv')\n",
    "# item_embs = torch.load(\"./saved_models/item_embs.pt\", map_location=torch.device(device))\n",
    "item_embs = None\n",
    "\n",
    "sasrec_config = dict(\n",
    "    manual_seed = 123,\n",
    "    sampler_seed = 123,\n",
    "    init_emb_svd = None,\n",
    "    lin_layer_dim = -1,\n",
    "    num_epochs = 5, #3 10 22 100&dropout0.9&hd32&bs1000\n",
    "    maxlen = 100,\n",
    "    hidden_units = 128,\n",
    "    dropout_rate = 0.3,\n",
    "    num_blocks = 2,\n",
    "    num_heads = 2,\n",
    "    batch_size = 128,\n",
    "    learning_rate = 1e-3,\n",
    "    fwd_type = 'bce',\n",
    "    l2_emb = 0,\n",
    "    patience = 10,\n",
    "    skip_epochs = 1,\n",
    "    n_neg_samples=600,\n",
    "    sampling='without_rep'\n",
    ")\n",
    "\n",
    "config = TrainConfig(\n",
    "    orthogonal_init = True,\n",
    "    q_n_hidden_layers = 1,\n",
    "    qf_lr = 3e-4,\n",
    "    batch_size=128,\n",
    "    device=\"cuda:0\",\n",
    "    bc_steps=100000,\n",
    "    cql_alpha=100.0,\n",
    "\n",
    "    env=\"MovieLens\",\n",
    "    project= \"CQL-SASREC\",\n",
    "    group= \"CQL-SASREC\",\n",
    "    name= \"CQL\",\n",
    "    #cql_negative_samples = 10\n",
    ")\n",
    "\n",
    "# model, _, _, _ = prepare_sasrec_model(base_config_bce, training_temp, data_description_temp, device, item_embs)\n",
    "# model.load_state_dict(torch.load('../sasrec_cql/RECE/saved_models/model_e2_nonsvd.pt', map_location=torch.device(device)))\n",
    "# model.eval()\n",
    "\n",
    "sasrec_model, _, _, optimizers = prepare_sasrec_model(sasrec_config,\n",
    "                                                      training_temp,\n",
    "                                                      data_description_temp,\n",
    "                                                      device,\n",
    "                                                      item_embs)\n",
    "\n",
    "# sasrec_model.fwd_type = 'embedding'\n",
    "\n",
    "model = prepare_cql_model(config, sasrec_model, data_description_temp, optimizers)\n",
    "model.load_state_dict(torch.load('./saved_models/sasrec_cql.pt', map_location=torch.device(device)))\n",
    "model.body.eval()\n",
    "model.q_1.eval()\n",
    "model.q_2.eval()\n",
    "\n",
    "model.item_emb = model.body.item_emb\n",
    "model.pad_token = sasrec_model.pad_token\n",
    "\n",
    "testset_, holdout_ = leave_one_out(\n",
    "    testset, target='timestamp', sample_top=True, random_state=0\n",
    ")\n",
    "\n",
    "test_size = 5000\n",
    "test_users = np.intersect1d(holdout_['userid'].unique(), testset_['userid'].unique())\n",
    "if test_size < len(test_users):\n",
    "    test_users  = np.random.choice(test_users, size=test_size, replace=False)\n",
    "testset = testset_[testset_['userid'].isin(test_users)].sort_values(by=['userid', 'timestamp'])\n",
    "holdout = holdout_[holdout_['userid'].isin(test_users)].sort_values(['userid'])\n",
    "\n",
    "get_test_scores(model, data_description_temp, testset, holdout, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
